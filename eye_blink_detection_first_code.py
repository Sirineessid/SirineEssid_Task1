# -*- coding: utf-8 -*-
"""Eye blink detection:first code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1znvDweZy-wxOWw_Dcr_qhdT11ld1zTtt
"""

!pip install opencv-python numpy dlib imutils

"""**Enable the Colab Virtual Webcam:**

"""

!pip install ipywebrtc
from ipywebrtc import CameraStream

!pip install ipywebrtc==0.6.0

!pip install ipywidgets
!jupyter nbextension enable --py widgetsnbextension

import cv2
cap = cv2.VideoCapture(0)  # Default camera
#cap = cv2.VideoCapture(1)



if not cap.isOpened():
    print("Cannot open camera")
    exit()

while True:
    # Capture frame-by-frame
    ret, frame = cap.read()
    if not ret:
        print("Can't receive frame (stream end?). Exiting ...")
        break

    # Display the resulting frame
    cv2.imshow('Webcam', frame)

    # Break loop with 'q'
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release the camera and close all OpenCV windows
cap.release()
cv2.destroyAllWindows()

!pip install --upgrade opencv-python opencv-python-headless

import cv2
print(cv2.VideoCapture(1).isOpened())
#cv2 does not access the camera

from IPython.display import Javascript

def capture_video():
    display(Javascript('''
        async function initCamera() {
            const video = document.createElement('video');
            video.setAttribute('autoplay', '');
            video.setAttribute('playsinline', '');
            video.style.width = '100%';
            video.style.height = '100%';
            document.body.appendChild(video);

            const stream = await navigator.mediaDevices.getUserMedia({video: true});
            video.srcObject = stream;
        }
        initCamera();
    '''))

!pip install opencv-python
!pip install opencv-python-headless

"""Facial Landmark Detection using **dlib**"""

# Importing the required dependencies
import cv2 # for video rendering
import dlib # for face and landmark detection
import imutils
import time
start_time = time.time()
timeout =10

# for calculating dist b/w the eye landmarks
from scipy.spatial import distance as dist

# to get the landmark ids of the left
# and right eyes ----you can do this
# manually too
from imutils import face_utils

cam = cv2.VideoCapture('assets/Video.mp4')

# Initializing the Models for Landmark and
# face Detection
detector = dlib.get_frontal_face_detector()

# Download the shape predictor model if it doesn't exist
import os
import urllib.request

model_path = 'shape_predictor_68_face_landmarks.dat'
if not os.path.exists(model_path):
    print("Downloading shape predictor model...")
    urllib.request.urlretrieve("https://github.com/italojs/facial-landmarks-recognition/raw/master/shape_predictor_68_face_landmarks.dat", model_path)
    print("Download complete.")

landmark_predict = dlib.shape_predictor(model_path)  # Updated path

while 1:

    # If the video is finished then reset it
    # to the start
    if cam.get(cv2.CAP_PROP_POS_FRAMES) == cam.get(
    cv2.CAP_PROP_FRAME_COUNT):
        cam.set(cv2.CAP_PROP_POS_FRAMES, 0)

    else:
        _, frame = cam.read()
        frame = imutils.resize(frame, width=640)

        # converting frame to gray scale to pass
        # to detector
        img_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # detecting the faces---#
        faces = detector(img_gray)
        for face in faces:
            cv2.rectangle(frame, face[0], face[1],
                        (200, 0, 0), 1)

        cv2.imshow("Video", frame)
        if cv2.waitKey(5) & 0xFF == ord('q'):
            break

cam.release()
cv2.destroyAllWindows()

"""# Eye Aspect Ratio (EAR)

EAR =

Eye Aspect Ratio=
(Sum of Verticle distnace)/
(2*Horizontal Distance of Eye)
"""

def calculate_EAR(eye):

    # calculate the vertical distances
    # euclidean distance is basically
    # the same when you calculate the
    # hypotenuse in a right triangle
    y1 = dist.euclidean(eye[1], eye[5])
    y2 = dist.euclidean(eye[2], eye[4])

    # calculate the horizontal distance
    x1 = dist.euclidean(eye[0], eye[3])

    # calculate the EAR
    EAR = (y1+y2) / x1

    return EAR

"""This is the most important part, when you calculate the EAR of an eye, it remains constant when the eye is open but it suddenly drops when the eye is blinked

Since we have two EAR for each eye respectively we’ll take the average of both the EAR for the right eye and the EAR for the left eye and then check if it is lower than a certain threshold ( we’ll create a variable to set its value) and this threshold might vary a bit, for me it worked with 0.4 or 0.5 but in some cases, it works with 0.25 or 0.3 as well. It depends on the FPS of your video or webcam.

All the Code is here
"""

# Importing the required dependencies
import cv2 # for video rendering
import dlib # for face and landmark detection
import imutils
# for calculating dist b/w the eye landmarks
from scipy.spatial import distance as dist
# to get the landmark ids of the left and right eyes
# you can do this manually too
from imutils import face_utils

# from imutils import

cam = cv2.VideoCapture('assets/my_blink.mp4')

# defining a function to calculate the EAR
def calculate_EAR(eye):

	# calculate the vertical distances
	y1 = dist.euclidean(eye[1], eye[5])
	y2 = dist.euclidean(eye[2], eye[4])

	# calculate the horizontal distance
	x1 = dist.euclidean(eye[0], eye[3])

	# calculate the EAR
	EAR = (y1+y2) / x1
	return EAR

# Variables
blink_thresh = 0.45
succ_frame = 2
count_frame = 0

# Eye landmarks
(L_start, L_end) = face_utils.FACIAL_LANDMARKS_IDXS["left_eye"]
(R_start, R_end) = face_utils.FACIAL_LANDMARKS_IDXS['right_eye']

# Initializing the Models for Landmark and
# face Detection
detector = dlib.get_frontal_face_detector()
landmark_predict = dlib.shape_predictor(
	'Model/shape_predictor_68_face_landmarks.dat')
while 1:

	# If the video is finished then reset it
	# to the start
	if cam.get(cv2.CAP_PROP_POS_FRAMES) == cam.get(
			cv2.CAP_PROP_FRAME_COUNT):
		cam.set(cv2.CAP_PROP_POS_FRAMES, 0)

	else:
		_, frame = cam.read()
		frame = imutils.resize(frame, width=640)

		# converting frame to gray scale to
		# pass to detector
		img_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

		# detecting the faces
		faces = detector(img_gray)
		for face in faces:

			# landmark detection
			shape = landmark_predict(img_gray, face)

			# converting the shape class directly
			# to a list of (x,y) coordinates
			shape = face_utils.shape_to_np(shape)

			# parsing the landmarks list to extract
			# lefteye and righteye landmarks--#
			lefteye = shape[L_start: L_end]
			righteye = shape[R_start:R_end]

			# Calculate the EAR
			left_EAR = calculate_EAR(lefteye)
			right_EAR = calculate_EAR(righteye)

			# Avg of left and right eye EAR
			avg = (left_EAR+right_EAR)/2
			if avg < blink_thresh:
				count_frame += 1 # incrementing the frame count
			else:
				if count_frame >= succ_frame:
					cv2.putText(frame, 'Blink Detected', (30, 30),
								cv2.FONT_HERSHEY_DUPLEX, 1, (0, 200, 0), 1)
				else:
					count_frame = 0

		cv2.imshow("Video", frame)
		if cv2.waitKey(5) & 0xFF == ord('q'):
			break

cam.release()
cv2.destroyAllWindows()

# Importing the required dependencies
import cv2  # for video rendering
import dlib  # for face and landmark detection
import imutils
from scipy.spatial import distance as dist  # for calculating distances
from imutils import face_utils

# Initializing video capture
cam = cv2.VideoCapture('assets/my_blink.mp4')

# Function to calculate the Eye Aspect Ratio (EAR)
def calculate_EAR(eye):
    y1 = dist.euclidean(eye[1], eye[5])
    y2 = dist.euclidean(eye[2], eye[4])
    x1 = dist.euclidean(eye[0], eye[3])
    EAR = (y1 + y2) / (2.0 * x1)
    return EAR

# Variables
blink_thresh = 0.45
succ_frame = 2
count_frame = 0
blink_count = 0  # Variable to count the total number of blinks

# Eye landmarks
(L_start, L_end) = face_utils.FACIAL_LANDMARKS_IDXS["left_eye"]
(R_start, R_end) = face_utils.FACIAL_LANDMARKS_IDXS["right_eye"]

# Initializing face and landmark detectors
detector = dlib.get_frontal_face_detector()
# Changed the path to the shape predictor model.
# Assuming it's in the same directory as the script.
landmark_predict = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')

while True:
    # Reset video if it reaches the end
    if cam.get(cv2.CAP_PROP_POS_FRAMES) == cam.get(cv2.CAP_PROP_FRAME_COUNT):
        cam.set(cv2.CAP_PROP_POS_FRAMES, 0)

    else:
        _, frame = cam.read()
        frame = imutils.resize(frame, width=640)
        img_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Detecting faces
        faces = detector(img_gray)
        for face in faces:
            shape = landmark_predict(img_gray, face)
            shape = face_utils.shape_to_np(shape)

            # Extract landmarks for both eyes
            lefteye = shape[L_start:L_end]
            righteye = shape[R_start:R_end]

            # Calculate EAR for both eyes
            left_EAR = calculate_EAR(lefteye)
            right_EAR = calculate_EAR(righteye)
            avg_EAR = (left_EAR + right_EAR) / 2.0

            # Detect blink based on EAR threshold
            if avg_EAR < blink_thresh:
                count_frame += 1
            else:
                if count_frame >= succ_frame:
                    blink_count += 1  # Increment blink count
                    cv2.putText(frame, f'Blink Detected', (30, 30),
                                cv2.FONT_HERSHEY_DUPLEX, 1, (0, 200, 0), 1)
                count_frame = 0

        # Display blink count on the video frame
        cv2.putText(frame, f'Blink Count: {blink_count}', (30, 60),
                    cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 255), 1)

        # Show the video
        cv2.imshow("Video", frame)
        if cv2.waitKey(5) & 0xFF == ord('q'):
            break

# Release resources
cam.release()
cv2.destroyAllWindows()
